# -*- coding: utf-8 -*-
"""Adobe_1b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vrjHlh9ql-ROAS2ucezwmodu9FfEr6aB
"""


import fitz
def extract_pdf_content(pdf_path):
    doc = fitz.open(pdf_path)
    text_by_page = []
    for i, page in enumerate(doc):
        text = page.get_text()
        text_by_page.append((i + 1, text))
    return text_by_page

from sentence_transformers import SentenceTransformer, util
import json
from datetime import datetime
import os

model = SentenceTransformer('all-MiniLM-L6-v2')

def rank_sections(text_by_page, persona, job):
    results = []
    query = f"{persona}. Task: {job}"
    query_emb = model.encode(query, convert_to_tensor=True)

    for page_num, text in text_by_page:
        sections = text.split('\n\n')
        for sec in sections:
            if len(sec.strip()) < 30:
                continue
            sec_emb = model.encode(sec.strip(), convert_to_tensor=True)
            score = util.cos_sim(query_emb, sec_emb).item()
            results.append({
                "page_number": page_num,
                "section_title": sec.strip().split('\n')[0][:80],
                "refined_text": sec.strip(),
                "similarity": score
            })
    results = sorted(results, key=lambda x: x['similarity'], reverse=True)
    return results

import re
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=-1)

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return ''.join(c if ord(c) < 128 else ' ' for c in text).strip()

def chunk_text(text, max_words=400):
    words = text.split()
    return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

def remove_persona_and_job(text, persona, job):
    text = re.sub(rf"(?i)\b{re.escape(persona)}\b", "", text)
    text = re.sub(rf"(?i)\b{re.escape(job)}\b", "", text)
    return text

def analyze_subsection(section_text, persona, job):
    section_text = clean_text(section_text)
    section_text = remove_persona_and_job(section_text, persona, job)
    chunks = chunk_text(section_text)

    all_summaries = []
    for chunk in chunks:
        try:
            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)
            all_summaries.append(summary[0]['summary_text'])
        except Exception as e:
            print("Error during summarization:", e)
            all_summaries.append("[Error summarizing this part]")

    return ' '.join(all_summaries)

from datetime import datetime
import os

def process_documents(pdf_paths, persona, job):
    final_output = {
        "metadata": {
            "input_documents": [os.path.basename(p) for p in pdf_paths],
            "persona": persona,
            "job_to_be_done": job,
            "processing_timestamp": datetime.now().isoformat()
        },
        "extracted_sections": [],
        "sub_section_analysis": []
    }

    for pdf_path in pdf_paths:
        content = extract_pdf_content(pdf_path)
        ranked = rank_sections(content, persona, job)
        for idx, section in enumerate(ranked[:]):
            final_output["extracted_sections"].append({
                "document": os.path.basename(pdf_path),
                "page_number": section["page_number"],
                "section_title": section["section_title"],
                "importance_rank": idx + 1
            })

            refined = analyze_subsection(section["refined_text"], persona, job)
            final_output["sub_section_analysis"].append({
                "document": os.path.basename(pdf_path),
                "page_number": section["page_number"],
                "refined_text": refined
            })

    return final_output

import os
persona = input("Enter the persona: ")
job = input("Enter the job: ")
pdf_dir = "./input"
pdf_paths = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
output_json = process_documents(pdf_paths, persona, job)

with open("./output/challenge1b_output.json", "w") as f:
    json.dump(output_json, f, indent=2)

from pprint import pprint
pprint(output_json)
